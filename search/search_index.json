{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About me","text":"<p>I'm Rolando Cabrera, a passionate Cloud Engineer with experience in High Performance Computing. With a strong background in Linux.</p>"},{"location":"#cv","title":"CV","text":"<p>Download my CV</p>"},{"location":"#linkedin","title":"LinkedIn","text":"<p>Please contact me at LinkedIn</p>"},{"location":"#mail","title":"Mail","text":"<p>Send me an email</p>"},{"location":"#certifications","title":"Certifications","text":""},{"location":"landing/","title":"Welcome to rolo5555","text":"<p>Hello World; This is a website that i use to post things of my interest.</p>"},{"location":"blog/","title":"Welcome to My Personal Blog!","text":"<p>This is the space where I share the solutions I\u2019ve discovered through trial and error. My goal is to explain them in the simplest and most intuitive way possible, making it easy for anyone to follow along and apply the same solutions.</p>"},{"location":"blog/2024/09/16/introduction-to-powertools-for-aws-lambda/","title":"Introduction to Powertools for AWS Lambda","text":"<p>Building serverless applications with AWS Lambda offers scalability, cost-efficiency, and reduced operational overhead. However, as these applications grow, so do the challenges.</p> <p>Have you struggled to pinpoint why a Lambda function slowed down or why your app returns intermittent errors? Debugging cryptic logs or understanding service interactions across distributed systems can be time-consuming and frustrating.</p> <p>These challenges lead to:</p> <ul> <li>Unhappy customers due to downtime or poor performance.</li> <li>Wasted developer hours troubleshooting avoidable issues.</li> <li>Critical visibility gaps that hinder optimization and troubleshooting.</li> </ul> <p>In a serverless environment, traditional monitoring methods fall short. You need tools designed for the serverless world.</p>"},{"location":"blog/2024/09/16/introduction-to-powertools-for-aws-lambda/#what-are-lambda-python-powertools","title":"What Are Lambda Python Powertools?","text":"<p>AWS Lambda Powertools for Python is an open-source library provided by AWS that simplifies and enhances the development of serverless applications written in Python. It is a collection of utilities designed to follow best practices and streamline tasks such as logging, tracing, metrics collection, and configuration management in AWS Lambda.</p>"},{"location":"blog/2024/09/16/introduction-to-powertools-for-aws-lambda/#core-features","title":"Core Features","text":"<ol> <li>Logging:</li> <li>Provides opinionated, structured JSON logging out of the box.</li> <li>Automatically captures contextual information, such as Lambda function name, version, and AWS request ID.</li> <li>Logs can include custom metadata, making them easier to analyze in tools like CloudWatch, Elasticsearch, or Splunk.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.logging import Logger\n\nlogger = Logger()\nlogger.info(\"This is a structured log\", extra={\"user_id\": \"12345\"})\n</code></pre> <p>Output:</p> <pre><code>{\n    \"level\": \"INFO\",\n    \"timestamp\": \"2024-11-22T10:00:00Z\",\n    \"message\": \"This is a structured log\",\n    \"user_id\": \"12345\",\n    \"function_name\": \"my-function\"\n}\n</code></pre> <ol> <li>Metrics:</li> <li>Simplifies the creation and publishing of custom metrics to Amazon CloudWatch.</li> <li>Supports dimensions, namespaces, and multiple metrics in a single Lambda execution.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.metrics import Metrics\n\nmetrics = Metrics(namespace=\"MyApplication\", service=\"PaymentService\")\nmetrics.add_metric(name=\"SuccessfulPayments\", unit=\"Count\", value=1)\nmetrics.publish()\n</code></pre> <ol> <li>Tracing:</li> <li>Provides integration with AWS X-Ray for distributed tracing.</li> <li>Automatically traces Lambda handler execution and external calls like DynamoDB, S3, or HTTP requests.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.tracing import Tracer\n\ntracer = Tracer()\n\n@tracer.capture_lambda_handler\ndef handler(event, context):\n    # Traced code\n    pass\n</code></pre> <ol> <li>Validation:</li> <li>Helps validate input events using Pydantic models or JSON schemas.</li> <li>Reduces boilerplate and ensures robust input validation.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.utilities.validation import validate\nfrom schema import MY_EVENT_SCHEMA\n\n@validate(event_schema=MY_EVENT_SCHEMA)\ndef handler(event, context):\n    # Validated input\n    pass\n</code></pre> <ol> <li>Parameters:</li> <li>Simplifies the retrieval of parameters from AWS Systems Manager Parameter Store, AWS Secrets Manager, AWS AppConfig, Amazon DynamoDB, and custom providers.</li> <li>Supports caching and transforming parameter values (JSON and base64), improving performance and flexibility.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.utilities.parameters import get_parameter\n\n# Retrieve a parameter from AWS Secrets Manager\nparameter = get_parameter(name=\"my-secret\", provider=\"SecretsManager\")\n\n# Retrieve multiple parameters with caching\nparameters = get_parameter(names=[\"param1\", \"param2\"], cache=True)\n\n# Retrieve and decode a base64 parameter\nbase64_param = get_parameter(name=\"my-base64-param\", base64_decode=True)\n</code></pre> <ol> <li>Event Source Data Classes:</li> <li>Provides self-describing classes for common Lambda event sources, helping with type hinting, code completion, and decoding nested fields.</li> <li>Simplifies working with event data by including docstrings for fields and providing helper functions for easy deserialization.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.utilities.event_handler import event_source\n\n# Event source for S3\nfrom aws_lambda_powertools.utilities.event_source import S3Event\n\ndef handler(event, context):\n    s3_event = S3Event(event)  # Type hinting and auto-completion\n\n    # Access event fields with decoding/deserialization\n    bucket_name = s3_event.records[0].s3.bucket.name\n    object_key = s3_event.records[0].s3.object.key\n    print(f\"Bucket: {bucket_name}, Object Key: {object_key}\")\n</code></pre> <ol> <li>Parser (Pydantic):</li> <li>Simplifies data parsing and validation using Pydantic to define data models, parse Lambda event payloads, and extract only the needed data.</li> <li>Offers runtime type checking and user-friendly error messages for common AWS event sources.</li> </ol> <p>Example:</p> <pre><code>from aws_lambda_powertools.utilities.parser import parse\nfrom pydantic import BaseModel\n\n# Define a data model using Pydantic\nclass MyEventModel(BaseModel):\n    user_id: int\n    user_name: str\n\n@parse(model=MyEventModel)\ndef handler(event, context):\n    # Access validated and parsed event data\n    print(f\"User ID: {event.user_id}, User Name: {event.user_name}\")\n</code></pre> <ol> <li>Other Utilities:</li> <li>Feature Flags: Manage runtime feature toggles using a configuration file or DynamoDB.</li> <li>Event Parser: Parse and validate common AWS event formats (e.g., DynamoDB, S3).</li> </ol>"},{"location":"blog/2024/09/16/introduction-to-powertools-for-aws-lambda/#why-use-lambda-powertools","title":"Why Use Lambda Powertools?","text":"<ol> <li>Faster Development: Reduces boilerplate code, letting you focus on business logic.</li> <li>Best Practices Built-In: Aligns with AWS Well-Architected Framework, especially for observability.</li> <li>Improved Observability: Standardizes logs, metrics, and traces for better debugging and monitoring.</li> <li>Production-Ready: Simplifies common patterns required in serverless applications, making them easier to maintain.</li> <li>Extensibility: Modular design allows you to include only the features you need.</li> </ol>"},{"location":"blog/2024/09/16/introduction-to-powertools-for-aws-lambda/#summary","title":"Summary","text":"<p>AWS Lambda Powertools for Python is like a Swiss Army Knife for serverless developers. It simplifies key operational tasks like logging, tracing, and metrics collection, ensuring best practices while improving developer productivity and code maintainability.</p>"},{"location":"blog/2024/09/20/devops-container/","title":"Devops container","text":""},{"location":"blog/2024/09/20/devops-container/#containers-solving-the-it-works-on-my-machine-problem","title":"Containers: Solving the \"It Works on My Machine\" Problem","text":"<p>Have you ever built an app that ran perfectly on your machine, only to see it crash and burn on someone else\u2019s? That\u2019s the classic \u201cIt works on my machine\u201d problem. Containers solve this by packaging everything your app needs\u2014code, libraries, tools, and settings\u2014so it behaves the same no matter where it's run.</p>"},{"location":"blog/2024/09/20/devops-container/#the-big-idea-bundle-every-dependency","title":"The Big Idea: Bundle Every Dependency","text":"<p>At the heart of containers is the idea of shipping all dependencies with your app. This isolates it from the host system and other apps, eliminating conflicts and surprises.</p>"},{"location":"blog/2024/09/20/devops-container/#but-containers-arent-magic","title":"But Containers Aren\u2019t Magic","text":"<p>They feel magical, but they\u2019re not. Containers are just regular Linux processes, given superpowers by kernel features.</p>"},{"location":"blog/2024/09/20/devops-container/#containers-processes","title":"Containers = Processes","text":"<p>A container is just a process with some isolation. The Linux kernel makes this happen.</p>"},{"location":"blog/2024/09/20/devops-container/#key-kernel-features","title":"Key Kernel Features","text":"<ul> <li>cgroups (control groups): limit and isolate resource usage (CPU, memory, etc.)</li> <li>namespaces: isolate things like process trees, users, network interfaces, and filesystems</li> </ul>"},{"location":"blog/2024/09/20/devops-container/#diving-deeper","title":"Diving Deeper","text":"<ul> <li>pivot_root: changes the root filesystem of a process</li> <li>layers: containers are built from stacked image layers, saving space and time</li> <li>overlay filesystems: combine multiple layers into one coherent view</li> <li>container registries: store and distribute container images (like Docker Hub)</li> </ul>"},{"location":"blog/2024/09/20/devops-container/#the-power-of-isolation","title":"The Power of Isolation","text":"<ul> <li>cgroups control resource limits</li> <li>namespaces isolate environments:</li> <li>PID, user, network, and mount namespaces create that sandboxed feeling</li> <li>how to make a namespace: tools like <code>unshare</code> or <code>clone()</code> can do it</li> </ul>"},{"location":"blog/2024/09/20/devops-container/#networking-security","title":"Networking &amp; Security","text":"<ul> <li>container IP addresses: every container can get its own IP</li> <li>capabilities: fine-grained control over what a process can do</li> <li>seccomp-BPF: filter syscalls for tighter security</li> </ul>"},{"location":"blog/2024/09/20/devops-container/#flexibility-through-config","title":"Flexibility Through Config","text":"<ul> <li>configuration options: containers are highly customizable through runtime configs</li> </ul>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/","title":"<code>Github Actions</code>: Automate Your Way to Success in Development","text":""},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#the-problem","title":"The problem","text":"<p>In today\u2019s fast-paced software development world, teams are constantly under pressure to deliver high-quality applications quickly. However, as applications grow in complexity, manual processes often become a bottleneck, leading to errors, delays, and inconsistent releases. This is where DevOps practices come into play, offering solutions to streamline and automate these processes.</p> <p>Let\u2019s split the problem into two distinct but intertwined realms: Continuous Integration (CI) and Continuous Deployment (CD). These practices are designed to help development teams avoid common pitfalls and improve the speed and reliability of software delivery.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>Humans are prone to errors: we forget dependencies, make linting mistakes, and work with complex systems that have interdependencies\u2014especially in large teams. These mistakes can lead to technical debt, production bugs, and negative impacts on end users, which is the last thing any developer wants.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#continuous-deployment-cd","title":"Continuous Deployment (CD)","text":"<p>With CD, the challenge is automating software updates. In a typical CD process, code is automatically built, tested, and then deployed. Manual intervention introduces risk, slows things down, and leaves room for mistakes. Automating the deployment pipeline ensures faster and more reliable software delivery.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#cast-of-characters","title":"Cast of characters","text":""},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#workflows","title":"Workflows","text":"<p>A workflow is a configurable automated process consisting of one or more jobs. These workflows are defined in <code>.github/workflows</code> and can be triggered by a variety of events. Common workflows include:</p> <ul> <li>Building and testing pull requests.</li> <li>Deploying applications after a release.</li> <li>Automatically adding labels to new issues.</li> </ul>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#events","title":"Events","text":"<p>Events are activities in a repository that trigger workflows. Examples include:</p> <ul> <li>Pushing code.</li> <li>Opening a pull request.</li> <li>Creating a new issue.</li> <li>Workflows can also be triggered manually or on a predefined schedule. For a full list, see GitHub\u2019s documentation on Events that trigger workflows.</li> </ul>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#jobs","title":"Jobs","text":"<p>A job is a collection of steps executed on the same runner. Each step can either be a shell script or an action that is part of the workflow. Steps are executed in order, and you can share data between them.</p> <p>Jobs run in parallel unless configured otherwise, meaning independent jobs can speed up processes. For example, you can have parallel build jobs for different architectures followed by a packaging job that only starts after all builds finish successfully.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#actions","title":"Actions","text":"<p>An action is a reusable application that performs specific tasks. Actions help reduce repetitive code in workflows. Common actions include:</p> <ul> <li>Pulling your Git repository.</li> <li>Setting up toolchains for building.</li> <li>Authenticating with cloud providers.</li> </ul> <p>You can write custom actions or use those available in the GitHub Marketplace.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#runners","title":"Runners","text":"<p>A runner is a server that executes your workflows. GitHub provides runners for Ubuntu, Windows, and macOS, which run in isolated virtual machines.</p> <p>You also have the option to host your own runner if you need a specific OS or hardware configuration.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#example-of-usage","title":"Example of Usage","text":"<p>Here's an example of a pipeline configuration for deploying a static website using GitHub Actions. This pipeline includes a <code>workflow</code> that automates the deployment process whenever code is pushed to the <code>main</code> or <code>master</code> branch.</p> <p>The workflow is triggered by two specific events: a push to the <code>main</code> or <code>master</code> branch. When triggered, the pipeline runs a <code>job</code> with several <code>steps</code>, each using <code>actions</code> to configure credentials, fetch the code, install dependencies, and deploy the website.</p> <pre><code>#.github/workflows/action.yml\nname: CD\non:\n  push:\n    branches:\n      - master\n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name &lt;cool_action_name&gt;\n          git config user.email &lt;cool_action_email&gt;\n      - uses: actions/setup-python@v5\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV\n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: pip install mkdocs-material[imaging] &amp;&amp; pip install mkdocs-material\n      - run: mkdocs gh-deploy --force\n</code></pre> <p>For more examples and detailed information, check the GitHub Actions Documentation.</p>"},{"location":"blog/2024/09/20/github-actions-automate-your-way-to-success-in-development/#conclusion","title":"Conclusion","text":"<p>This post aimed to introduce the basics of CI/CD and demonstrate how it can streamline your development process. By automating workflows like the one shown above, you can improve the speed and reliability of your deployments. CI/CD isn't just for large teams\u2014it's a valuable tool for developers of all sizes and stages to enhance code quality and simplify deployment :). </p>"},{"location":"blog/2025/03/20/engineering-dbdiagram/","title":"Engineering dbdiagram","text":""},{"location":"blog/2025/03/20/engineering-dbdiagram/#designing-relational-databases-visually-with-dbdiagramio","title":"Designing Relational Databases Visually with dbdiagram.io","text":"<p>When planning a relational database, the hardest part isn\u2019t writing SQL\u2014it\u2019s designing the schema. Tables, relationships, primary/foreign keys\u2026 it\u2019s easy to lose track of how everything connects. That\u2019s where dbdiagram.io shines.</p>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#what-is-dbdiagramio","title":"\ud83e\udde9 What is dbdiagram.io?","text":"<p>dbdiagram.io is a free, web-based tool for designing and visualizing relational databases. Instead of manually drawing ERDs or keeping schemas in your head, you can quickly build diagrams that represent your database structure\u2014visually and with code.</p>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#why-use-it","title":"\u2728 Why Use It?","text":"<ul> <li>Visual-first: See your tables and relationships laid out clearly</li> <li>Fast to write: Uses a simple DSL (Domain-Specific Language) to define tables</li> <li>Shareable: Easy to share links or export diagrams for documentation</li> <li>Import/Export: Works with PostgreSQL, MySQL, SQL Server, and more</li> <li>Great for collaboration: Perfect for design discussions or onboarding new devs</li> </ul>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#how-it-works","title":"\u2699\ufe0f How It Works","text":"<p>Here\u2019s a quick example:</p> <pre><code>Table users {\n  id int [pk]\n  name varchar\n  email varchar\n}\n\nTable posts {\n  id int [pk]\n  user_id int [ref: &gt; users.id]\n  title varchar\n  content text\n}\n</code></pre> <p>This defines two tables (<code>users</code> and <code>posts</code>) with a one-to-many relationship.</p> <p>The UI then auto-generates a clean, interactive diagram.</p>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#import-from-sql","title":"\ud83d\udce5 Import from SQL","text":"<p>Already have a schema? Paste your SQL directly, and dbdiagram will parse it for you. Supported dialects include:</p> <ul> <li>PostgreSQL</li> <li>MySQL</li> <li>SQLite</li> <li>SQL Server</li> </ul>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#export-options","title":"\ud83d\udce4 Export Options","text":"<p>You can export your diagrams as:</p> <ul> <li>SQL files (for running in a DB)</li> <li>PNG/SVG images (for documentation)</li> <li>PDF (for reports or specs)</li> </ul>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#use-cases","title":"\ud83c\udf10 Use Cases","text":"<ul> <li>Prototyping database designs quickly</li> <li>Documenting existing databases visually</li> <li>Teaching or learning database design</li> <li>Improving dev\u2013design\u2013stakeholder collaboration</li> </ul>"},{"location":"blog/2025/03/20/engineering-dbdiagram/#final-thoughts","title":"\ud83e\udde0 Final Thoughts","text":"<p>dbdiagram.io is a simple yet powerful tool to bring clarity to your database designs. Whether you're sketching a new schema or trying to understand an existing one, it's a must-have for any backend dev or data engineer.</p> <p>Give it a spin at dbdiagram.io and see your databases come to life.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/","title":"How to Estimate Projects Effectively","text":""},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#the-problem","title":"The Problem","text":"<p>During my experience working with consulting companies in the IT tech industry, project estimation has always been a rough task. Whether it's a new development, a migration, or a modernization effort, determining the time and effort required is never straightforward. There are many challenges, including incomplete requirements, unknowns, and even misaligned expectations between clients and engineering teams.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#advice-considerations","title":"Advice &amp; Considerations","text":""},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#1-dont-reinvent-the-wheel","title":"1. Don't Reinvent the Wheel","text":"<p>One of the biggest mistakes in estimation is failing to acknowledge what has already been developed. Before jumping into a solution, take the time to assess existing frameworks, libraries, or tools that could speed up the process. Reusability can significantly reduce effort and risk.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#2-take-your-time-understanding-the-problem","title":"2. Take Your Time Understanding the Problem","text":"<p>Gathering all the requirements is crucial. Sometimes, teams rush into estimations without fully understanding the scope of the work. This often results in underestimating or overestimating tasks. Ask the right questions, clarify any ambiguities, and ensure all stakeholders are aligned before finalizing an estimate.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#3-challenge-requests-when-necessary","title":"3. Challenge Requests When Necessary","text":"<p>Many times, we are asked to implement solutions that may not be ideal due to a lack of knowledge from the requester. It\u2019s essential to understand the real needs behind a request and provide insights or alternative approaches when necessary. While the final decision is on the client, you should always express your thoughts and ensure technical feasibility and best practices are followed.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#4-identify-assumptions-unknowns","title":"4. Identify Assumptions &amp; Unknowns","text":"<p>There will always be assumptions and unknowns in any estimation. Clearly document them so that any potential risks are highlighted early. If possible, assign buffers to account for uncertainties, and update estimates as more information becomes available.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#5-remove-unnecessary-layers-of-communication","title":"5. Remove Unnecessary Layers of Communication","text":"<p>Often, information is gathered from secondary sources, acting as proxies between the original client and the engineering team. This can lead to misinterpretations and missing context. Whenever possible, reduce the number of intermediaries and obtain information directly from the source.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#6-compare-learn-from-past-estimations","title":"6. Compare &amp; Learn from Past Estimations","text":"<p>One of the best ways to improve estimation skills is by continuously learning from past experiences. Compare your estimated efforts against the actual time spent, identify gaps, and refine your estimation process accordingly. Understanding previous mistakes and missing elements can help fine-tune future estimates.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#7-seek-client-feedback","title":"7. Seek Client Feedback","text":"<p>Engaging with the client for feedback can provide valuable insights into the accuracy of your estimates. If possible, ask for feedback at various stages of the project to ensure expectations are met and to adjust estimations for similar projects in the future.</p>"},{"location":"blog/2025/03/20/how-to-estimate-projects-effectively/#conclusion","title":"Conclusion","text":"<p>Project estimation is a challenging but essential skill in IT consulting. By considering existing solutions, fully understanding the problem, questioning requests when necessary, and learning from past projects, you can significantly improve your ability to provide accurate and realistic estimates. Remember, estimation is not just about guessing numbers\u2014it\u2019s about applying knowledge, experience, and critical thinking to create reliable plans.</p>"},{"location":"blog/2024/09/13/kube-bench-automating-kubernetes-security-checks/","title":"<code>Kube-bench</code>: Automating Kubernetes Security Checks","text":""},{"location":"blog/2024/09/13/kube-bench-automating-kubernetes-security-checks/#what-is-it","title":"What is it?","text":"<p>The Center for Internet Security (CIS) has established benchmarks to ensure the secure deployment of Kubernetes clusters. These benchmarks provide security configuration guidelines for Kubernetes, aiming to help organizations protect their environments from potential vulnerabilities.</p> <p>One tool that automates this important process is <code>kube-bench</code>. It runs checks against your Kubernetes cluster based on the CIS Kubernetes Benchmark, helping to verify whether your cluster is configured securely.</p>"},{"location":"blog/2024/09/13/kube-bench-automating-kubernetes-security-checks/#why-use-kube-bench","title":"Why Use Kube-bench?","text":"<p><code>kube-bench</code> streamlines security auditing by automating the verification of your Kubernetes setup. It checks for best practices, identifies misconfigurations, and reports areas where your setup might fall short of CIS recommendations. This makes it easier to maintain compliance and reduce the risk of exposure to security threats.</p> <p>Whether you're running Kubernetes in production, or setting up a development cluster, regular use of kube-bench helps ensure that your deployments meet security standards.</p> <p>For more details and to start using kube-bench, visit the official GitHub repository.</p>"},{"location":"blog/2024/09/13/kube-bench-automating-kubernetes-security-checks/#running-kube-bench-on-kubernetes-clusters","title":"Running kube-bench on Kubernetes Clusters","text":"<p>The <code>kube-bench</code> tool can be executed in various ways depending on your Kubernetes cluster setup. It ensures that your Kubernetes deployment aligns with the CIS Kubernetes Benchmark, which provides security guidelines.</p> <p>In this blog, I\u2019ll share how I used kube-bench to audit both worker and master nodes of a Kubernetes cluster deployed with kOps on AWS.</p>"},{"location":"blog/2024/09/13/kube-bench-automating-kubernetes-security-checks/#worker-node-auditing","title":"Worker Node Auditing","text":"<p>To audit the worker nodes, I submitted a Kubernetes job that runs <code>kube-bench</code> specifically for worker node configuration. Below are the steps:</p> <pre><code># Download the worker node job configuration\n$ curl -O https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job.yaml\n\n$ kubectl apply -f job.yaml\njob.batch/kube-bench created\n\n$ kubectl get pods\nNAME                      READY   STATUS              RESTARTS   AGE\nkube-bench-j76s9   0/1     ContainerCreating   0          3s\n\n# Wait for a few seconds for the job to complete\n$ kubectl get pods\nNAME                      READY   STATUS      RESTARTS   AGE\nkube-bench-j76s9   0/1     Completed   0          11s\n\n# The results are held in the pod's logs\nkubectl logs kube-bench-j76s9\n[INFO] 4 Worker Node Security Configuration\n[INFO] 4.1 Worker Node Configuration Files\n...\n</code></pre> <p>The logs will contain a detailed list of recommendations, outlining the identified security issues and how to address them. You can see an example of the full output in this Gist.</p> <p>Within the output, each problematic area is explained, and kube-bench offers solutions for improving security on the worker nodes.</p>"},{"location":"blog/2024/09/13/kube-bench-automating-kubernetes-security-checks/#master-node-auditing","title":"Master Node Auditing","text":"<p>To audit the master nodes (control plane), I used a script specifically designed for the master node configuration. Follow these steps to run the audit:</p> <pre><code># Download the master node job configuration\n$ curl -O https://raw.githubusercontent.com/aquasecurity/kube-bench/main/job-master.yaml\n\n$ kubectl apply -f job-master.yaml\njob.batch/kube-bench created\n\n$ kubectl get pods\nNAME                      READY   STATUS              RESTARTS   AGE\nkube-bench-xxxxx   0/1     ContainerCreating   0          3s\n\n# Wait for a few seconds for the job to complete\n$ kubectl get pods\nNAME                      READY   STATUS      RESTARTS   AGE\nkube-bench-xxxxx   0/1     Completed   0          11s\n\n# The results are held in the pod's logs\nkubectl logs kube-bench-j76s9\n[INFO] 1 Control Plane Security Configuration\n[INFO] 1.1 Control Plane Node Configuration Files\n...\n</code></pre> <p>The logs will contain a detailed list of recommendations, outlining the identified security issues and how to address them. You can see an example of the full output in this Gist.</p>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/","title":"Helm Charts: Kubernetes Deployments Made Simple","text":"<p>Managing applications in Kubernetes can get messy\u2014fast. Think YAML files everywhere, lots of <code>kubectl apply</code>, and debugging when something\u2019s off. That\u2019s where Helm comes in.</p>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#what-is-helm","title":"\ud83d\udee0\ufe0f What is Helm?","text":"<p>Helm is the package manager for Kubernetes. Think of it like <code>apt</code> for Ubuntu or <code>brew</code> for macOS\u2014but for your Kubernetes apps.</p> <p>Instead of juggling dozens of YAML files, you define a Helm chart, which is a reusable template for your application, including:</p> <ul> <li>Deployments</li> <li>Services</li> <li>ConfigMaps</li> <li>Ingress</li> <li>and more...</li> </ul> <p>A Helm chart helps you install, upgrade, and manage complex Kubernetes apps with just a few commands.</p>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#why-use-helm","title":"\u2705 Why Use Helm?","text":"<ul> <li>Reusability: One chart can serve multiple environments (dev, staging, prod) by swapping values.</li> <li>Consistency: Templates reduce copy-paste errors across YAMLs.</li> <li>Versioning: Helm tracks releases and lets you roll back easily.</li> <li>Simplicity: You can install an entire app stack (like Prometheus, NGINX, etc.) with one command.</li> </ul>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#using-a-helm-chart-quick-start","title":"\ud83d\ude80 Using a Helm Chart (Quick Start)","text":""},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#1-install-helm","title":"1. Install Helm","text":"<pre><code>brew install helm   # macOS\n# or\nsudo apt install helm  # Debian/Ubuntu\n</code></pre>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#2-add-a-chart-repo","title":"2. Add a Chart Repo","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#3-install-an-app-eg-postgresql","title":"3. Install an App (e.g., PostgreSQL)","text":"<pre><code>helm install my-postgres bitnami/postgresql\n</code></pre> <p>This will deploy PostgreSQL into your Kubernetes cluster using a pre-built chart.</p>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#4-customize-with-values","title":"4. Customize with Values","text":"<p>Create a <code>values.yaml</code> file to override default settings:</p> <pre><code>auth:\n  username: myuser\n  password: mypass\n  database: mydb\n</code></pre> <p>Then install with:</p> <pre><code>helm install my-postgres bitnami/postgresql -f values.yaml\n</code></pre>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#5-upgrade-or-roll-back","title":"5. Upgrade or Roll Back","text":"<pre><code>helm upgrade my-postgres bitnami/postgresql -f values.yaml\nhelm rollback my-postgres 1\n</code></pre>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#6-uninstall","title":"6. Uninstall","text":"<pre><code>helm uninstall my-postgres\n</code></pre>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#writing-your-own-helm-chart-optional-teaser","title":"\ud83d\udce6 Writing Your Own Helm Chart (Optional Teaser)","text":"<p>You can also create your own chart with:</p> <pre><code>helm create mychart\n</code></pre> <p>This generates a full chart scaffold. Then you just modify templates and <code>values.yaml</code> to fit your app.</p>"},{"location":"blog/2025/03/26/helm-charts-kubernetes-deployments-made-simple/#final-thoughts","title":"\ud83e\udde0 Final Thoughts","text":"<p>Helm brings structure, repeatability, and sanity to your Kubernetes workflows. Whether you're managing a small service or a full production stack, Helm lets you ship faster with fewer headaches.</p>"},{"location":"blog/2024/09/13/kubesec-static-analysis-security-scanning-tool/","title":"<code>kubesec</code> Static analysis security scanning tool.","text":""},{"location":"blog/2024/09/13/kubesec-static-analysis-security-scanning-tool/#the-problem","title":"The problem","text":"<p>Kubernetes resources can be vulnerable to misconfigurations, leading to security risks in your infrastructure. Detecting these issues early is critical to maintaining a secure environment.</p>"},{"location":"blog/2024/09/13/kubesec-static-analysis-security-scanning-tool/#what-is","title":"What is ?","text":"<p>Is an open-source tool that performs static analysis on Kubernetes resources, identifying security risks before deployment. It ensures that your Kubernetes configuration adheres to best security practices.</p>"},{"location":"blog/2024/09/13/kubesec-static-analysis-security-scanning-tool/#how-to-usage-kubesec","title":"How to usage <code>kubesec</code>","text":"<p>There are several ways to use <code>kubesec</code> to scan your Kubernetes resources:</p> <ul> <li>Docker container image: <code>docker.io/kubesec/kubesec:v2</code></li> <li>Linux/MacOS/Win binary (get the latest release)</li> <li>Kubernetes Admission Controller</li> <li>Kubectl plugin</li> </ul>"},{"location":"blog/2024/09/13/kubesec-static-analysis-security-scanning-tool/#using-the-docker-image","title":"Using the Docker Image","text":"<p>The simplest way to run kubesec is by using its Docker image and passing the file you want to scan. For example, to check the <code>app1-510d6362.yaml</code> file:</p> <pre><code>docker run -i kubesec/kubesec:512c5e0 scan /dev/stdin &lt; app1-510d6362.yaml\n</code></pre> <p>This command runs a scan on the specified file, producing results like this:</p> <pre><code>[\n  {\n    \"object\": \"Pod/pod.default\",\n    \"valid\": true,\n    \"message\": \"Passed with a score of 0 points\",\n    \"score\": 0,\n    \"scoring\": {\n      \"advise\": [\n        {\n          \"selector\": \"containers[] .securityContext .readOnlyRootFilesystem == true\",\n          \"reason\": \"An immutable root filesystem can prevent malicious binaries being added to PATH and increase attack cost\"\n        },\n        {\n          \"selector\": \"containers[] .securityContext .runAsNonRoot == true\",\n          \"reason\": \"Force the running image to run as a non-root user to ensure least privilege\"\n        },\n        {\n          \"selector\": \"containers[] .securityContext .runAsUser -gt 10000\",\n          \"reason\": \"Run as a high-UID user to avoid conflicts with the host's user table\"\n        },\n        {\n          \"selector\": \"containers[] .securityContext .capabilities .drop\",\n          \"reason\": \"Reducing kernel capabilities available to a container limits its attack surface\"\n        },\n        {\n          \"selector\": \"containers[] .securityContext .capabilities .drop | index(\\\"ALL\\\")\",\n          \"reason\": \"Drop all capabilities and add only those required to reduce syscall attack surface\"\n        },\n        {\n          \"selector\": \"containers[] .resources .requests .cpu\",\n          \"reason\": \"Enforcing CPU requests aids a fair balancing of resources across the cluster\"\n        },\n        {\n          \"selector\": \"containers[] .resources .limits .cpu\",\n          \"reason\": \"Enforcing CPU limits prevents DOS via resource exhaustion\"\n        },\n        {\n          \"selector\": \"containers[] .resources .requests .memory\",\n          \"reason\": \"Enforcing memory requests aids a fair balancing of resources across the cluster\"\n        },\n        {\n          \"selector\": \"containers[] .resources .limits .memory\",\n          \"reason\": \"Enforcing memory limits prevents DOS via resource exhaustion\"\n        },\n        {\n          \"selector\": \".spec .serviceAccountName\",\n          \"reason\": \"Service accounts restrict Kubernetes API access and should be configured with least privilege\"\n        },\n        {\n          \"selector\": \".metadata .annotations .\\\"container.seccomp.security.alpha.kubernetes.io/pod\\\"\",\n          \"reason\": \"Seccomp profiles set minimum privilege and secure against unknown threats\"\n        },\n        {\n          \"selector\": \".metadata .annotations .\\\"container.apparmor.security.beta.kubernetes.io/nginx\\\"\",\n          \"reason\": \"Well defined AppArmor policies may provide greater protection from unknown threats. WARNING: NOT PRODUCTION READY\"\n        }\n      ]\n    }\n  }\n]\n</code></pre>"},{"location":"blog/2024/09/16/managing-linux-dotfiles-a-guide-to-customizing-your-environment/","title":"Managing Linux Dotfiles: A Guide to Customizing Your Environment","text":""},{"location":"blog/2024/09/16/managing-linux-dotfiles-a-guide-to-customizing-your-environment/#introduction","title":"Introduction","text":"<p>Dotfiles are hidden configuration files in Unix-based systems (including Linux) that store settings for various applications and shell environments. They allow users to personalize their workflow, customize command-line tools, and manage configurations across multiple machines. In this post, we\u2019ll explore what dotfiles are, why they\u2019re important, and how to efficiently manage them for a seamless Linux experience.</p> <p>Whether you\u2019re new to dotfiles or looking for advanced techniques to manage them across multiple systems, this guide will cover everything you need to know.</p>"},{"location":"blog/2024/09/16/managing-linux-dotfiles-a-guide-to-customizing-your-environment/#why-using-dotfiles","title":"Why using dotfiles","text":"<ul> <li> <p>Productivity: Automate system and tool setup, saving time and effort when configuring new machines. Dotfiles instantly apply your preferred settings.</p> </li> <li> <p>Consistency: Keep a uniform development environment across all devices, whether using macOS, Linux, or Windows, ensuring efficiency regardless of platform.</p> </li> <li> <p>Shareable: Share your dotfiles with the community or use others' configurations, enabling collaboration and faster setup for new tools and languages.</p> </li> <li> <p>Backup: Version control your dotfiles to back up and restore configurations easily, ensuring you can quickly recover your environment when needed.</p> </li> </ul>"},{"location":"blog/2024/09/16/managing-linux-dotfiles-a-guide-to-customizing-your-environment/#dotfiles","title":"dotfiles","text":"<p>https://github.com/mathiasbynens/dotfiles</p> <p>https://github.com/holman/dotfiles</p>"},{"location":"blog/2024/09/16/managing-linux-dotfiles-a-guide-to-customizing-your-environment/#conclusion","title":"Conclusion","text":"<p>Dotfiles are powerful tools for personalizing and optimizing your Linux environment. By organizing and version-controlling them, you can ensure your workflow remains consistent across different machines. Start simple by creating your dotfiles, and as you become more comfortable, explore automation and symlink management for even greater efficiency.</p> <p>Don\u2019t forget to back up and share your dotfiles\u2014it\u2019s an excellent way to maintain consistency and collaborate with other developers!</p>"},{"location":"blog/2024/09/16/mi-primer-cluster/","title":"Mi primer cluster","text":"<pre><code>Me acuerdo el d\u00eda que del cloud yo me enamor\u00e9\nMi viejo lleg\u00f3 sonriendo, me dijo que ya deploy\u00e9\nLe ped\u00ed otro cluster y le dije que el de kubear me lo s\u00e9\nMe dijo: \"De una, ingeniero, si no, \u00bfc\u00f3mo vas a escalar?\"\nSi el cluster me llama, yo lo voy a escalar\nT\u00fa me conoces, yo no soy un novato\nFirm\u00e9 un commit en un repo de GitLab\nMe apunt\u00e9 en un proyecto que a\u00fan no tiene backup\nSabes que yo deployo infra\nD\u00eda y noche yo monitoreo\nDevOps reconoce a DevOps\nSin Kubernetes nac\u00ed y con Kubernetes me muero\nComo mi jefe, de la nube al cielo\nTanta infra que gestion\u00e9, microservicios hay que optimizar\nOye, \u00bfcu\u00e1ndo cae mi bonus en el payroll?\nBusco autoscaling\nD\u00eda y noche yo deployeo\nDevOps reconoce a DevOps\nSin Terraform nac\u00ed y con Terraform me muero\n</code></pre>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/","title":"<code>trivy</code> Automating Security Scanning Applied to Terraform Resources","text":""},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#introduction","title":"Introduction","text":"<p>Security is a vital concern when managing infrastructure, and it\u2019s critical to identify vulnerabilities in both container images and infrastructure-as-code (IaC). While Terraform helps automate the deployment of cloud resources, combining it with security tools like <code>trivy</code> ensures that any configuration or resource vulnerabilities are caught early.</p> <p>In this post, we will walk through how to integrate <code>trivy</code> into your Terraform workflow to automate security scanning of the resources you define. We will cover setting up <code>trivy</code>, running scans, and interpreting the results, ensuring your Terraform-managed infrastructure is as secure as possible.</p>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#use-case","title":"Use case","text":"<p>It\u2019s important to recognize that Trivy is a versatile security tool capable of scanning a wide range of resources, including container images, file systems, and repositories. However, in this post, we will focus specifically on scanning Infrastructure as Code (IaC) through Terraform configuration, utilizing Trivy\u2019s misconfiguration scanning mode.</p> <p>The Terraform configuration scanning feature is accessible through the <code>trivy config command</code>. This command performs a comprehensive scan of all configuration files within a directory to detect any misconfiguration issues, ensuring your infrastructure is secure from the start. You can explore more details on misconfiguration scans within the Trivy documentation, but here we\u2019ll focus on two primary methods: scanning Terraform plans and direct configuration files.</p>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#method-1-scanning-with-a-terraform-plan","title":"Method 1: Scanning with a Terraform Plan","text":"<p>The first method involves generating a Terraform plan and scanning it for misconfigurations. This allows Trivy to assess the planned infrastructure changes before they are applied, giving you the opportunity to catch issues early.</p> <pre><code>cd $DESIRED_PATH\nterraform plan --out tfplan\ntrivy config tfplan\n</code></pre> <ul> <li>The <code>terraform plan --out tfplan</code> command creates a serialized Terraform plan file.</li> <li><code>trivy config tfplan</code> then scans this plan for any potential security risks, providing insights before applying the configuration.</li> </ul>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#method-2-scanning-configuration-files-directly","title":"Method 2: Scanning Configuration Files Directly","text":"<p>Alternatively, you can scan the Terraform configuration files directly without generating a plan. This is useful when you want to perform quick checks on your existing code or infrastructure definitions.</p> <pre><code>cd $DESIRED_PATH\ntrivy config ./ \n</code></pre> <p>This command instructs Trivy to recursively scan all Terraform files in the specified directory, reporting any misconfigurations found.</p>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#trivy-installation","title":"Trivy installation","text":"<p>For installation instructions please refer to the oficial documentation</p>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#see-it-in-action","title":"See it in action","text":""},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#automating-the-scans-in-a-cicd-pipeline","title":"Automating the Scans in a CI/CD Pipeline","text":"<p>A good strategy is integrating trivy scans into your CI/CD pipeline. As an example we can expose it through github Actions, the official action can be found here,but as an easy alternative this pipe can be definied:</p> <pre><code># GitHub Actions YAML file\nname: Terraform Security Scanning\n\non: [push]\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout repository\n      uses: actions/checkout@v2\n\n    - name: Install trivy\n      run: |\n        curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sudo sh\n\n    - name: Run trivy scan\n      run: trivy config --severity HIGH,CRITICAL --exit-code 1 .\n\n</code></pre>"},{"location":"blog/2024/09/20/trivy-automating-security-scanning-applied-to-terraform-resources/#conclusion","title":"Conclusion","text":"<p>Summarize the importance of security scanning in the Terraform workflow and how using trivy automates this process. Encourage readers to integrate scanning tools into their infrastructure deployments for proactive vulnerability management.</p>"},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/","title":"<code>terraform-docs</code>: A Cool Way of Documenting Terraform Projects","text":""},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#what-is-it","title":"What Is It?","text":"<p>terraform-docs is a utility that generates documentation from Terraform modules in various output formats. It allows you to easily integrate documentation that displays inputs, outputs, requirements, providers, and more! It supports several output formats\u2014my personal favorite is Markdown.</p>"},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#what-does-it-look-like","title":"What Does It Look Like?","text":"<p>An example from the official documentation provides a clear illustration of how the module works, making it much easier for users to understand its usage. Below is an image demonstrating this example.</p> Screenshot of a markdown table generated by `terraform-docs`."},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#installation","title":"Installation","text":"<p>As the installation process may change over time, please refer to the official documentation.</p>"},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#options","title":"Options","text":"<p>You need to define a <code>.config</code> directory inside the directory where you want to generate the documentation. In this file, we define:</p> <ul> <li><code>formatter</code>: Set to Markdown, which I recommend.</li> <li><code>output</code>: Set to <code>README.md</code>, which is the default file for displaying content in a repository.</li> <li><code>sort</code>: To enable sorting of elements. We use the required criteria that sorts inputs by name and shows required ones first.</li> <li><code>settings</code>: General settings to control the behavior and the generated output.</li> <li><code>content</code>: The specific content to include in the documentation.</li> </ul>"},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#minimal-configuration","title":"Minimal Configuration","text":"<pre><code>formatter: \"markdown\"\n\noutput:\n  file: \"README.md\"\n\nsort:\n  enabled: true\n  by: required\n\nsettings:\n  read-comments: false\n  hide-empty: true\n  escape: false\n\ncontent: |-\n  {{ .Requirements }}\n\n  {{ .Modules }}\n\n  {{ .Inputs }}\n\n  {{ .Outputs }}\n</code></pre> <p>For more details about the configuration, please refer to this guide</p>"},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#integration-with-github-actions","title":"Integration with Github Actions","text":"<p>To use <code>terraform-docs</code> with GitHub Actions, configure a YAML workflow file (e.g., <code>.github/workflows/documentation.yml</code>) with the following content:</p> <pre><code>name: Generate terraform docs\n\non:\n  pull_request:\n    branches:\n      - main\n\njobs:\n  terraform:\n    name: \"terraform docs\"\n    runs-on: ubuntu-latest\n\n    # Use the Bash shell regardless whether the GitHub Actions runner is ubuntu-latest, macos-latest, or windows-latest\n    defaults:\n      run:\n        shell: bash\n\n    steps:\n      # Checkout the repository to the GitHub Actions runner\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      # Install the latest version of Terraform CLI\n      - name: Check docs\n        uses: terraform-docs/gh-actions@v1.0.0\n        with:\n          output-file: README.md\n          fail-on-diff: true\n</code></pre>"},{"location":"blog/2024/09/10/terraform-docs-a-cool-way-of-documenting-terraform-projects/#see-it-in-action","title":"See it in action","text":"<p>Here's an example of <code>terraform-docs</code> being used in a personal module I developed.</p> <p> </p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/","title":"<code>Terragrunt</code> Raise the DRY flag","text":"<p>If you're familiar with Infrastructure as Code (IaC) tools, this post is for you. The goal here is to introduce you to <code>Terragrunt</code>, a tool that enables you to follow the DRY (Don't Repeat Yourself) principle, making your Terraform code more maintainable and concise.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#what-it-is","title":"What it is?","text":"<p><code>Terragrunt</code> is a flexible orchestration tool designed to scale Infrastructure as Code, making it easier to manage Terraform configurations across multiple environments.</p> <p>Let's present the problem.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#keep-your-backend-configuration-dry","title":"Keep your backend configuration DRY","text":"<p>Before diving into <code>Terragrunt</code>, let's first define the problem it solves. Consider the following Terraform project structure:</p> <pre><code>#./terraform/\n.\n\u251c\u2500\u2500 envs\n\u2502   \u251c\u2500\u2500 dev\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 main.tf\n\u2502   \u251c\u2500\u2500 prod\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u2514\u2500\u2500 main.tf\n\u2502   \u2514\u2500\u2500 stage\n\u2502       \u251c\u2500\u2500 backend.tf\n\u2502       \u2514\u2500\u2500 main.tf\n\u2514\u2500\u2500 modules\n    \u2514\u2500\u2500 foundational\n        \u2514\u2500\u2500 main.tf\n</code></pre> <p>In this scenario, we have a foundational module, with separate environments for dev, stage, and prod. As the complexity of the system grows, maintaining repeated backend configurations becomes more challenging.</p> <p>Take, for example, the following backend configuration for the dev environment:</p> <pre><code># ./terraform/envs/dev/backend.tf\nterraform {\n  backend \"s3\" {\n    bucket         = \"my-terraform-state\"\n    key            = \"envs/dev/bakcend/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"my-lock-table\"\n  }\n}\n</code></pre> <p>This configuration is required for each environment (<code>dev</code>, <code>stage</code>, <code>prod</code>), and you'll find yourself copying the same code across all of them. This approach isn't scalable and quickly becomes difficult to maintain.</p> <p>Now, let's see how <code>Terragrunt</code> simplifies this.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#introducing-terragrunt","title":"Introducing Terragrunt","text":"<p>With <code>Terragrunt</code>, you can move repetitive backend configurations into a single file and reuse them across all environments. </p> <p>Here's how your updated directory structure looks:</p> <pre><code># ./terraform/\n.\n\u251c\u2500\u2500 envs\n\u2502   \u251c\u2500\u2500 dev\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500terragrunt.hcl\n\u2502   \u251c\u2500\u2500 prod\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl\n\u2502   \u251c\u2500\u2500 stage\n\u2502   \u2502   \u251c\u2500\u2500 backend.tf\n\u2502   \u2502   \u251c\u2500\u2500 main.tf\n\u2502   \u2502   \u2514\u2500\u2500 terragrunt.hcl\n\u2514\u2500\u2500 terragrunt.hcl\n</code></pre> <p>The <code>terragrunt.hcl</code> file uses the same HCL language as Terraform and centralizes the backend configuration. Instead of duplicating code, we now use Terragrunt\u2019s <code>path_relative_to_include()</code> function to dynamically set the backend key for each environment. </p> <p>Here\u2019s what that looks like:</p> <pre><code>#./terraform/envs/terragrunt.hcl\nremote_state {\n  backend = \"s3\"\n  generate = {\n    path      = \"backend.tf\"\n    if_exists = \"overwrite_terragrunt\"\n  }\n  config = {\n    bucket = \"my-terraform-state\"\n\n    key = \"${path_relative_to_include()}/terraform.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    dynamodb_table = \"my-lock-table\"\n  }\n}\n</code></pre> <p>By centralizing this, you only need to update the root <code>terragrunt.hcl</code> to apply changes across all environments.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#including-root-configuration","title":"Including root configuration","text":"<p>You can include the root configuration in each child environment by referencing the root <code>terragrunt.hcl</code> file like this:</p> <pre><code>#./terraform/env/stage/terragrunt.hcl\ninclude \"root\" {\n  path = find_in_parent_folders()\n}\n</code></pre> <p>This drastically reduces duplication and keeps your backend configurations DRY.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#keep-your-provider-configuration-dry","title":"Keep your provider configuration DRY","text":"<p>One common challenge in managing Terraform configurations is dealing with repetitive provider blocks. For example, when you're configuring AWS provider roles, you often end up with the same block of code repeated across multiple modules:</p> <pre><code># ./terraform/env/stage/main.tf\nprovider \"aws\" {\n  assume_role {\n    role_arn = \"arn:aws:iam::0123456789:role/terragrunt\"\n  }\n}\n</code></pre> <p>To avoid copy-pasting this configuration in every module, you can introduce Terraform variables:</p> <pre><code># ./terraform/env/stage/main.tf\nvariable \"assume_role_arn\" {\n  description = \"Role to assume for AWS API calls\"\n}\n\nprovider \"aws\" {\n  assume_role {\n    role_arn = var.assume_role_arn\n  }\n}\n</code></pre> <p>This approach works fine initially, but as your infrastructure grows, maintaining this configuration across many modules can become tedious. For instance, if you need to update the configuration (e.g., adding a <code>session_name</code> parameter), you would have to modify every module where the provider is defined.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#simplify-with-terragrunt","title":"Simplify with Terragrunt","text":"<p>Terragrunt offers a solution to this problem by allowing you to centralize common Terraform configurations. Like with backend configurations, you can define the provider configuration once and reuse it across multiple modules. Using Terragrunt\u2019s generate block, you can automate the creation of provider configurations for each environment.</p> <p>Here\u2019s how it works:</p> <pre><code>#./terraform/env/stage/terragrunt.hcl\ngenerate \"provider\" {\n  path = \"provider.tf\"\n  if_exists = \"overwrite_terragrunt\"\n  contents = &lt;&lt;EOF\nprovider \"aws\" {\n  assume_role {\n    role_arn = \"arn:aws:iam::0123456789:role/terragrunt\"\n  }\n}\nEOF\n}\n</code></pre> <p>This generate block tells Terragrunt to create a provider.tf file in the working directory (where Terragrunt calls Terraform). The provider.tf file is generated with the necessary AWS provider configuration, meaning you no longer need to manually define this in every module.</p> <p>When you run a Terragrunt command like terragrunt plan or terragrunt apply, it will generate the provider.tf file in the local .terragrunt-cache directory for each module</p> <pre><code>$ cd /terraform/env/stage/\n$ terragrunt apply\n$ find . -name \"provider.tf\"\n.terragrunt-cache/some-unique-hash/provider.tf\n</code></pre> <p>This approach ensures that your provider configuration is consistent and automatically injected into all relevant modules, saving you time and effort while keeping your code DRY.</p> <p>By centralizing provider configurations with Terragrunt, you reduce the risk of errors from manual updates and ensure that any changes to provider settings are automatically applied across all modules.</p>"},{"location":"blog/2024/09/10/terragrunt-raise-the-dry-flag/#installation","title":"Installation","text":"<p>For installation instructions, please refer to the official documentation</p>"},{"location":"blog/archive/2025/","title":"March 2025","text":""},{"location":"blog/archive/2024/","title":"September 2024","text":""},{"location":"blog/category/kubernetes/","title":"Kubernetes","text":""},{"location":"blog/category/engineering/","title":"Engineering","text":""},{"location":"blog/category/devops/","title":"Devops","text":""},{"location":"blog/category/terraform/","title":"Terraform","text":""},{"location":"blog/category/cloud/","title":"Cloud","text":""},{"location":"blog/category/linux/","title":"Linux","text":""},{"location":"blog/category/memes/","title":"Memes","text":""},{"location":"blog/page/2/","title":"Welcome to My Personal Blog!","text":""}]}